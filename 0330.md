# 0330
## 언어지능 딥러닝
### 1교시
### 아침 뉴스
#### 대규모 AI 개발 6개월 중단 촉구 메시지
- 대규모 AI를 적극 활용하고 개발하는 직업을 할 것인가?
- 아니면 대규모 AI가 할 수 없는 분야로 갈 것인가?
#### 독특한 테마 공부를 하고 싶은 분을 위하여
1. Casual Inference
2. Baysian Learning
3. Graph Neural Network
4. Reinforcement Learning

### Text Classification & Embedding
- 서로 다른 길이의 텍스트를 어떻게 모델에 넣을 것인가
- 긍정 / 부정을 위해 one-hot encoding, categorical, softmax...

#### 과거 테크닉
- Bag of words
    - 모든 유니크한 단어를 모델 인풋에..
    - 용량?????
    - 순서는?
    - 인간의 어학적인 지식을 많이 포함한 전처리
- Tf idf
- sparse한 전략들
---
### 2교시
#### New Strategy
- Word Embedding layer & sequence model
    1. tokenizing: 의미가 있는 최소한의 단위 token(index)로 분할
    2. text seq -> idx seq
    3. 문장 길이 통일
        - padding
        - truncating
    - Height: unique한 토큰 수
    - Width: Feature Dimension. 단어 하나하나가 특징값들로 representation 되기 위해
- 딥러닝과 텍스트의 첫 만남
- 긍정/부정 분류만 하는거면 조금 제한적이네..
---
### 3교시
#### 해결해야 할 것
1. tokenize를 어떻게
    - 어학적
    - 중요하지 않은 것
    - 필요한 것만
2. embedding을 어떻게
    - padding / trimming 위치?
3. modeling을 어떻게
    - 학습 가능하도록 vectorize
    - 그리고 학습시켜
---