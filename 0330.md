# 0330
## 언어지능 딥러닝
### 1교시
### 아침 뉴스
#### 대규모 AI 개발 6개월 중단 촉구 메시지
- 대규모 AI를 적극 활용하고 개발하는 직업을 할 것인가?
- 아니면 대규모 AI가 할 수 없는 분야로 갈 것인가?
#### 독특한 테마 공부를 하고 싶은 분을 위하여
1. Casual Inference
2. Baysian Learning
3. Graph Neural Network
4. Reinforcement Learning

### Text Classification & Embedding
- 서로 다른 길이의 텍스트를 어떻게 모델에 넣을 것인가
- 긍정 / 부정을 위해 one-hot encoding, categorical, softmax...

#### 과거 테크닉
- Bag of words
    - 모든 유니크한 단어를 모델 인풋에..
    - 용량?????
    - 순서는?
    - 인간의 어학적인 지식을 많이 포함한 전처리
- Tf idf
- sparse한 전략들
---
### 2교시
#### New Strategy
- Word Embedding layer & sequence model
    1. tokenizing: 의미가 있는 최소한의 단위 token(index)로 분할
    2. text seq -> idx seq
    3. 문장 길이 통일
        - padding
        - truncating
    - Height: unique한 토큰 수
    - Width: Feature Dimension. 단어 하나하나가 특징값들로 representation 되기 위해
- 딥러닝과 텍스트의 첫 만남
- 긍정/부정 분류만 하는거면 조금 제한적이네..
---
### 3교시
#### 해결해야 할 것
1. tokenize를 어떻게
    - 어학적
    - 중요하지 않은 것
    - 필요한 것만
2. embedding을 어떻게
    - padding / trimming 위치?
3. modeling을 어떻게
    - 학습 가능하도록 vectorize
    - 그리고 학습시켜
---
### 4교시
#### Tokenize
- ``from tensorflow.keras.preprocessing.text import Tokenizer``
    - ``tokenizer = Tokenizer(num_words=max_words, lower=False)``
    - num_words: 총 단어 개수, 상위 n개 // 패딩용 0을 기본으로 넣기 때문에 실제로는 n-1개
    - lower: 대소문자 구분 여부, 한국어는 False로
    - ``tokenizer.fit_on_texts(x_train)``: 데이터로 토큰화 학습
    - ``x_train = tokenizer.texts_to_sequences(x_train)``: text to sequence
- ``from tensorflow.keras.preprocessing.sequence import pad_sequences``: 패딩
    - ``pad_sequences(x_train, maxlen=max_len)``: 문장 최대 길이 설정, 패딩
        - ``padding='pre'``: 디폴트. 앞 쪽 패딩 / post는 뒤
        - ``truncating``: padding과 same

#### Embedding
- ``from tensorflow.keras.layers import Embedding``
    - ``Embedding( input_dim: max_words, output_dim: embedding_dim, input_length=max_len )``
        - input_dim: 총 단어 개수(=max_words)
        - output_dim: 단어 임베딩 차원
        - input_length: 인풋 길이(=max_len)
    - **다른 사람이 만든 Embedding 값도 사용 가능!**

#### Modeling
```python
il = Input(shape=(max_len,))
hl = Embedding(max_words, embedding_dim, input_length=max_len)(il)
hl = Conv1D(64, 5, activation='swish')(hl)
hl = Bidirectional(layer=LSTM(32, return_sequences=True),
                   backward_layer=LSTM(32, return_sequences=True, go_backwards=True))(hl)
hl = Bidirectional(layer=GRU(32, return_sequences=True),
                   backward_layer=SimpleRNN(16, return_sequences=True, go_backwards=True))(hl)
hl = Conv1D(32, 5, activation='swish')(hl)
hl = MaxPool1D(2)(hl)
hl = Flatten()(hl)
hl = Dense(1024, activation='swish')(hl)
ol = Dense(1, activation='sigmoid')(hl)

model = Model(il, ol)
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics = ['accuracy'])
```
---
### 5교시
- 코드 실습
- functional로 모델링할 시 Embedding 레이어 이전에 별도의 Input 레이어 필요. shape는 Embedding의 input_length
---
### 6교시
### Text Generation
- 첫 단어를 입력받아서 다음 말 예측하기
- 두 번째 단어를 입력받아서 다음 말 예측하기
- 한 문장을 쪼개서 바로 다음 단어를 y값으로
- 단어들을 단어 가방에 저장
- 어떻게 모델링하나? **분류!** (생성이 아니라 분류!!)
- 데이터 -> 인덱스 시퀀스 -> 원핫인코딩
- 명시적인 원핫인코딩을 피할 수 있는 방법은?
    - ``sparse_categorical_crossentropy``
---
### 7교시
- Text Generation 실습