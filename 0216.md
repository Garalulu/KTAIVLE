# 0216
1교시
## 웹크롤링

### Server & Client Architecture
- Client
    - 브라우저를 통해 데이터 요청
- Server
    - Client가 데이터를 요청하면 요청에 따라 데이터 전송

``http://news.naver.com:80/main/read.nhn?mode=LSD&mid=shm&sid1=105&oid=001&aid=0009847211#da_727145 ``
- protocol ``http``: http, https 등 클라이언트-서버간 규약
- domain ``news.naver.com``: 링크 주소. DNS가 IP주소로 변환해줌. naver.com
- port ``:80``: 어떤 서비스를 이용할 지 선택함. 80, 443 - web, 3306 - DB
- path ``/main/``: 어떤 디렉토리로 들어갈지
- page ``read.nhn``: 디렉토리 안에 있는 파일
- query ``?mode=LSD&mid=shm&sid1=105&oid=001&aid=0009847211``: 특정 조건에 맞추어 검색
- fragment ``#da_727145``: 페이지 특정 위치로 이동

### Get & Post
- Get
    - URL에 데이터 포함(노출)
    - 길이 제한
- Post
    - Body에 데이터 포함(숨겨짐)

### Internet
- RFC 791: 통신 프로토콜을 이용하여 정보를 주고받는 네트워크
- 해저케이블을 이용하여 전세계 서버에 접속

### OSI 7 Layer
- 데이터를 보낼 땐 추가적인 정보를 담아야 한다
---
2교시

### Cookie & Session & Cache
- Cookie
    - Client에서 저장하는 문자열 데이터로 도메인별로 따로 저장
    - 하나에 클라이언트에 300개, 도메인당 20개, 쿠키 하나당 4Kbyte
- Session
    - Server에 저장하는 객체 데이터, 브라우저와 연결시 Session ID 생성
    - Session ID를 Cookie에 저장함으로써 로그인 연결 유지
- Cache
    - Client나 Server의 메모리에 저장하여 빠르게 데이터 로드하기 위한 저장소

### HTTP Status Code
- Server - Client 데이터 주고받은 결과 상태 코드
- 2xx: Success
- 3xx: redirection (browser cache)
- 4xx: request error
- 5xx: server error

### Web Language & Framework
- Client 
    - HTML
    - CSS - less, sass 
    - Javascript - vue.js, react.js, angelar.js, barkborn.js 
- Server
    - Python - Django, Flask
    - Java - Spring // 정부 표준
    - Ruby - Rails
    - Javascript - Nodejs (Express)
    - Scala - Play

### 웹 페이지 종류
- 정적 페이지: 로드 후 화면 변경 없음 // json 파싱
- 동적 페이지: 로드 후에도 이벤트 발생시 변경 // html 파싱
- 브라우저 직접 열어서 데이터 파싱 // selenium
- 빠른 순서: json > html > selenium

https://hackertyper.net/# 해커인척 하기

---
3교시

### 웹크롤링 과정
1. 웹서비스 분석: url
    - pc 페이지가 복잡하면 모바일에서
    - 개발자 도구에서 Network - Fetch/XHR에서 페이지 클릭 후 나오는 동적 페이지 요청 URL 복사
2. 서버에 데이터 요청: request(url) > response: json(str)
    - ``requests.get(url)``
3. 서버에서 받은 데이터 파싱: json(str) > list, dict > dataframe
    - ``response.json()``으로 json -> list
    - ``pd.Dataframe()``으로 list -> dataframe

- financedatareader 사용하면 편리함
- python tip:
    - ``help(함수)``로 함수 사용 방법 확인하기 (``alt+F12``)
    - ``dir(데이터)``로 사용 가능 메소드 확인하기
    - 함수 만들때 `` ''' ''' ``로 함수 설명 써주기
---
4교시

- 데이터 전처리
    - 데이터 타입 변경
    - ``df.dtypes``: 데이터 타입 확인
    - ``.replace(',', '')``: 쉼표 제거
 ```python
 usd_df['closePrice'].apply(lambda data: float(data.replace(',',''))) # 쉼표 제거 후 float형으로 변경
 ```
- lambda: 파라미터와 리턴으로 이루어진 함수를 간단하게 작성하는 방법
```python
def plus(n1, n2)
    return n1 + n2
plus_lambda = lambda n1, n2: n1 + n2
```
- 얕은 복사, 깊은 복사
    - 대입자(``=``)로만 넣으면 얕은 복사, 원본 위치 참조만 함 = 주소복사
    - ``.copy()``를 해야 깊은 복사, 새로운 데이터 생성

- 데이터 전처리(이어서)
    - 데이터 스케일링
        - min-max scaling
        - ``from sklearn.preprocessing import minmax_scale``
        - ``minmax_scale()``: 데이터를 0~1 사이 값으로 변경해줌
---
5교시

### 웹크롤링 - 403 에러
- 서버 쪽에서 유저의 요청을 거부하는 경우
- user-agent 추가하기
```python
headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'
}
response = requests.get(url, headers=headers)
```
- referer(바로 전에 요청한 URL) 추가하기
    - headers에 referer 값 추가
    - Daum 금융은 headers, referer가 있어야 에러 없이 작동함

### 크롤링 정책
- ``robots.txt``: 해당 페이지 크롤링 정책을 설명하는 페이지
    - ``User-agent: *``: 모든 유저가 (명시되어 있을 경우 해당 유저가)
    - ``disallow: /``: 모든 루트 (명시되어 있을 경우 해당 루트)에서 크롤링 금지
- 따로 크롤링법이 존재하지 않으므로 따르지 않더라도 위법은 아니나 과도한 크롤링으로 서비스에 영향을 주었을 경우 업무방해 혐의로 고소당할 수 있음
- 법적 문제: 지적재산권, 서비스과부화, 데이터 사용 표준

### API 사용법
- 데이터를 가지고 있는 업체에서 데이터를 가져갈 수 있도록 하는 서비스
---
6교시

1. Client - APP 등록, Server - Key 전달
    - ID와 시크릿 키 저장
2. API URL 확인
    - URL / 헤더 값(-H) / Param (-d) 확인
```python
ko_txt = '웹크롤링은 재미있습니다!'
url = 'https://openapi.naver.com/v1/papago/n2mt'
headers = {
    'Content-Type': 'application/json',
    'charset': 'UTF-8',
    'X-Naver-Client-Id': CLIENT_ID,
    'X-Naver-Client-Secret': CLIENT_SECRET
}
params = {
    'source': 'ko',
    'target': 'en',
    'text': ko_txt
}
```
3. Client - Key값과 함께 URL로 요청, Server - Key 확인 후 데이터 전달
``` python
response = requests.post(url, json.dumps(params), headers=headers)
```
- ``json.dumps()``를 사용해야 한국어를 특수문자로 바꾸어 전송할 수 있다.
- 에러가 발생한다면 항상 ``response.text``를 보고 오류 메시지를 확인하자
4. Data -> DataFrame
- Naver Papago 번역
- ``response.json()['message']['result']['translatedText']``

```python
def translate(txt, source='ko', target='en'):
    CLIENT_ID, CLIENT_SECRET = CLIENT_ID, CLIENT_SECRET
    url = 'https://openapi.naver.com/v1/papago/n2mt'
    headers = {
        'Content-Type': 'application/json',
        'charset': 'UTF-8',
        'X-Naver-Client-Id': CLIENT_ID,
        'X-Naver-Client-Secret': CLIENT_SECRET
    }
    params = {
        'source': source,
        'target': target,
        'text': txt
    }
    response = requests.post(url, json.dumps(params), headers=headers)
    return response.json()['message']['result']['translatedText']
```
7교시

``%config InlineBackend.figure_formats = {'png', 'retina'}`` : 선명한 그래프 그리기

```python
params = {
    'startDate': '2018-01-01',
    'endDate': '2023-01-01',
    'timeUnit': 'month',
    'keywordGroups': [
        {'groupName': '트위터', 'keywords': ['트위터', '트윗']},
        {'groupName': '페이스북', 'keywords': ['페이스북', '페북']},
        {'groupName': '인스타그램', 'keywords': ['인스타그램', '인스타']},
    ],
}
```
파라미터 내 여러 속성이 있는 경우 []로 묶고 안에서 속성 하나마다 {}로 묶음


### 결론 
- API가 없다면? 네트워크 트래픽에서 링크를 찾는다(동적 페이지)
- API가 있다면? Key값과 시크릿키를 받아서 API 문서에서 제공하는 방식대로 요청한다