# 0228
## 딥러닝
### 1교시
#### Review
```python
# import
import tensorflow as tf
from tensorflow import keras

# clear
keras.backend.clear_session()

# define
model = keras.models.Sequential()

# shape
model.add(keras.layers.Input(shape=(4,)))
model.add(keras.layers.Dense(3))
# model.add(keras.layers.Dense(3, activation='softmax'))
## model.add(keras.layers.Dense(1, activation='sigmoid'))

# compile
model.compile(loss='mse', optimizer='adam')
# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
## model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# fit
model.fit(x_train, y_train, epochs=10, verbose=1)

# predict
y_pred = model.predict(x_test)
```
---
### 2교시
#### 히든 레이어
- ``Dense(x, activation='relu')``
- relu: x가 0 이하면 0, 0 초과면 x값 그대로 출력
- 히든 레이어가 뭐길래 성능을 갑자기 올려줄까? 나중에 확인
---
### 3교시
#### to_categorical
```python
from tensorflow.keras.utils import to_categorical

class_n = len(np.unique(y_train))

y_train = to_categorical(y_train, class_n)
y_test = to_categorical(y_test, class_n)
```
- 엄밀하게 인코딩하는 방법
---
### 4교시
- 딥러닝은 학습시 초기 가중치값을 랜덤하게 부여하기 때문에 결과가 달라진다
- 학습이 잘 되었다 = 에러를 줄여나가는 방향으로 가중치 업데이트가 잘 되었다
- 여러 특성을 하나(혹은 여러)의 정보로 요약
- ... 만약 학습이 정말 잘 된 상태라면 ``z = w1x1 + w2x2 + w0에서 z``
    - 계산이 간단한가? Input - Output 바로 연결된 연산은 매우 간단. 1차 다항식이기 때문
---
### 5교시
- 성능상 유용한가? yes.
    - 설명상(납득 가능하게 설명) 유용한가? yes. z값을 봐도 ^y값을 알 수 있기 때문!
- ... ``^y=sigmoid(z)``
    - z값 변화에 따라 ^y값이 결정됨
- ... 히든 레이어가 있는 상황에서 여전히 학습이 잘 된 상태라면
    - 첫 번째 히든 레이어 4번째 노드
    - ``1st z4 = 1st w1 4th * x1 + 1st w2 4th * x2 + 1st w0 4th``
    - ``1st H4 = sigmoid(1st z4)``
    - 계산이 간단한가? 연산량이 많지 않다.
    - 성능상 유용한가? 학습이 잘 되었기 때문에 유용하다 말 할 수 있다.
    - 설명상 유용한가? 
- ... 2번째 히든 레이어 4번째 노드
    - ``2nd z4 = 2nd w1 4th * 1st H1 + 2nd w2 4th * 1st H2 + 2nd w3 4th * 1st H3 + 2nd w4 4th * 1st H4 + 2nd w0 4th``
    - ``2nd H4 = sigmoid(2nd z4)``
    - 계산이 간단한가? 컴퓨터 기준 간단
    - 성능상 유용한가? 유용함(학습이 잘 되었으니)
    - 설명상 유용한가? 추상적이다..
- 히든레이어가 2개 층 뿐인데도 각 노드가 구체적으로 어떤 역할을 하는지 설명이 어려웠다
    - 설명이 그렇게 중요한가? 문제만 잘 해결되면 되는 것 아닌가?
1. 레이어가 많을수록 Low Level ~ High Level의 특징을 새롭게(기존에 없는) 뽑아내는 것
2. 노드의 수는 해당 Level에서 추출하려는 Feature의 수
3. High Level로 갈수록 기존에 없던 새로운 특징이 추출된다
- low level에서 노드 하나 추가했는데 성능이 좋아졌다 -> 영향을 끼치는 새로운 특징을 발견했다
- high level에서 노드 하나 제거했는데 성능이 변함없다 -> 불필요한 특징이었다
- 연결된 것으로부터 기존에 없던 새로운 feature를 잘 추출해낸 것 = **Feature Representation (Feature Learning)** // 딥러닝의 핵심 개념
    - 사람이 알려주는 게 아니라 컴퓨터가 알아서 특징을 추출
---
### 6교시
#### Feature Engineering
- 주어진 데이터를 의사결정에 도움되도록 만들어보는 절차
- 머신 러닝은 사람이 Feature extraction 하나, 딥러닝은 모든게 컴퓨터로 동작

- 이미 잘 정제된 데이터 기준으로는 사람이 특징 추출하기 쉬워 ML이나 DL이나 큰 차이 없다.
- 하지만 그렇지 않은 데이터라면? 이미지, 시 등등..

- Andrew Ng교수의 딥러닝 강의 들어보기
    - https://www.boostcourse.org/ai215
- 밑바닥부터 시작하는 딥러닝 책 

### MNIST
#### Early Stopping
```python
es = EarlyStopping(monitor='val_loss',
                   min_delta=0,
                   patience=5,
                   verbose=1,
                   restore_best_weights=True)
```
- ``from tensorflow.keras.callbacks import EarlyStopping``
    - ``monitor='val_loss'``: 관측 대상
    - ``min_delta``: threshold. loss가 얼마나 떨어져야 학습 진행되었다고 보는지
    - ``patience``: 성능 개선되지 않아도 몇번 참을래
    - ``restore_best_weight=True``: 가장 성능이 좋았던 epochs의 가중치를 쓸래
        - 이거 기본설정이 False인데 반드시 True로 바꾸고 하기!!
#### fit
- ``model.fit(train_x, train_y, validation_split=0.2, callbacks=[es], verbose=1, epochs=50)``
---
### 7교시
- validation_split: 트레이닝 셋을 fit하는 과정에서 자동으로 검증 데이터를 나누어 확인

- 레이어 노드 개수: 관습적으로 2의 제곱수를 많이 사용

https://bo-10000.tistory.com/75 : 컴퓨터 GPU 쓰기

### 활성함수
- Sigmoid: vanishing gradient problem 발생
- ReLU: 각종 파생함수 있음

#### min-max scaling
- 0~255 -> 0~1