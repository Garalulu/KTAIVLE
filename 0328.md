# 0328
## 언어지능 딥러닝
### 1교시
### 딥러닝의 기본
1. 최적화 수학
    - 언제 시작?
    1. Tensorflow / PyTorch Framework 개발자 (난이도: 상)
        - AutoGrad / AutoDiff
    2. 기존 모델을 변형하여 새로운 구조 디자인 (난이도: 중)
        - Gradient Check // Analytic vs Numerical
    3. Deep Learning의 근본 연구
        - Geoffrey hinton
    4. Hyperparameter tuning / Meta Learning
2. 구조에 대한 기능적인 이해
    - **현실에 응용 / 적용된 새로운 구조를 디자인해야만 할 때**
        - 기존의 유명한 구조를 해석할 능력
        - attention, transform까지
        - 그 뒤로는 대규모 모델이라 우선순위 밀림

#### 직업
1. AI 개발자
    - 대기업 취업용
2. AI 연구자 (AI 만능 잡무맨)
    - 빨리 벌고 빨리 은퇴하자!
    - Start-up
    - 대학원

#### Back propagation은 언제 공부하나?
- 취직하고 나서!
- 만약 진짜 지금 공부하고 싶다면..
1. CS231N 강의 - Andrej Karpathy
    - http://cs231n.stanford.edu/2021/schedule.html
    - Matrix: 나중에
    - Vector: 우선
---
### 2교시
#### ``return_sequences=False`` 이후 RNN 쌓을 시 에러 나는 이유?
- keras 구조상 발생하는 문제
- (전체 데이터 개수, 시점 수, 시점 당 feature 수)를 인풋으로 받아야 함
- 하지만 ``return_sequences=False``로 넘길 경우 keras 편의상 시점 수가 하나이므로 별도로 건네지 않음 (전체 데이터 개수, 시점당 feature 수)
- RNN 쌓을 때 인풋 차원 수가 달라 에러 발생

#### RNN 가중치 Counting
1. 카운팅
    - 이전 히든 스테이트 연결
    - 지금 인풋으로부터 연결
    - bias용 1개
    - 가중치가 같다: 똑같은 feature representation
    - 가중치가 다르다: 다른 feature representation
2. 가중치 재사용
    - 카운팅된 가중치는 재사용된다.
    - 히든 스테이트마다 동일한 가중치를 가짐
    - 총 가중치 수가 한 시점에서 가중치 * 시점이 아님!!!!
    - 한 시점에서 사용된 가중치가 모든 시점에서 사용
    - CNN은 공간에서 Filter가 / RNN은 시점에서 Hidden State가 동일한 가중치 사용
---
