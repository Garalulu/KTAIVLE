# 0328
## 언어지능 딥러닝
### 1교시
### 딥러닝의 기본
1. 최적화 수학
    - 언제 시작?
    1. Tensorflow / PyTorch Framework 개발자 (난이도: 상)
        - AutoGrad / AutoDiff
    2. 기존 모델을 변형하여 새로운 구조 디자인 (난이도: 중)
        - Gradient Check // Analytic vs Numerical
    3. Deep Learning의 근본 연구
        - Geoffrey hinton
    4. Hyperparameter tuning / Meta Learning
2. 구조에 대한 기능적인 이해
    - **현실에 응용 / 적용된 새로운 구조를 디자인해야만 할 때**
        - 기존의 유명한 구조를 해석할 능력
        - attention, transform까지
        - 그 뒤로는 대규모 모델이라 우선순위 밀림

#### 직업
1. AI 개발자
    - 대기업 취업용
2. AI 연구자 (AI 만능 잡무맨)
    - 빨리 벌고 빨리 은퇴하자!
    - Start-up
    - 대학원

#### Back propagation은 언제 공부하나?
- 취직하고 나서!
- 만약 진짜 지금 공부하고 싶다면..
1. CS231N 강의 - Andrej Karpathy
    - http://cs231n.stanford.edu/2021/schedule.html
    - Matrix: 나중에
    - Vector: 우선
---
### 2교시
#### ``return_sequences=False`` 이후 RNN 쌓을 시 에러 나는 이유?
- keras 구조상 발생하는 문제
- (전체 데이터 개수, 시점 수, 시점 당 feature 수)를 인풋으로 받아야 함
- 하지만 ``return_sequences=False``로 넘길 경우 keras 편의상 시점 수가 하나이므로 별도로 건네지 않음 (전체 데이터 개수, 시점당 feature 수)
- RNN 쌓을 때 인풋 차원 수가 달라 에러 발생

#### RNN 가중치 Counting
1. 카운팅
    - 이전 히든 스테이트 연결
    - 지금 인풋으로부터 연결
    - bias용 1개
    - 가중치가 같다: 똑같은 feature representation
    - 가중치가 다르다: 다른 feature representation
2. 가중치 재사용
    - 카운팅된 가중치는 재사용된다.
    - 히든 스테이트마다 동일한 가중치를 가짐
    - 총 가중치 수가 한 시점에서 가중치 * 시점이 아님!!!!
    - 한 시점에서 사용된 가중치가 모든 시점에서 사용
    - CNN은 공간에서 Filter가 / RNN은 시점에서 Hidden State가 동일한 가중치 사용
---
### 3교시
- RNN 실습
---
### 4교시
#### RNN 마무리
- 마지막 시점일수록 조금 더 중요히 여길 수 밖에 없는 구조
- 순서를 '중요'하게 여기는 모델: 데이터의 '순서'에 따라 영향을 받는 모델

### 언어
- 한국말: 미괄. 중요한걸 나중에
    - 조사 때문에 순서가 꼬여도 이해할 수 있다
- 영어: 두괄. 중요한걸 먼저
- 이처럼 특징이 다른 언어들 때문에 범용적인 모델을 만들기 쉽지 않음
- 아랍어
    - 언어에는 '모음' 존재
    - 문자에는 '모음' 없음
    - 모음이 몇 개 없어서 끼워맞추다 보면 다 보임
        - 나중에 모음 추가하면 안되나? -> 종교적인 이유로 불가능
- 히브리어 (아랍어와 유사)
    - YHWH: 신의 진명을 사람의 입으로 담을 수 없다

#### 비즈니스 현장
- Big data vs Application on 'specific' data / 'specialized' data
---
### 5교시
### 용어
- y bar: 평균
- y hat: 예측/추론
- y tilde: y가 될 수 있는 후보(candidate) // 편의를 위한 공학적
- 곱셈, tanh: 가중치 업뎃
- 덧셈: 같은 feature끼리 합산

### LSTM: Long Short Term Memory
- 기억 C와 Hidden state H 존재
- Forget Gate: 기억 망각
    - 기존 hidden state와 신규 input으로 시그모이드 함수 사용하여 각 feature들의 중요도 측정. 과거로부터 온 기억 C와 곱연산하는데 쓰임
        - 중요하다고 판단하면 중요도 up, 아니면 down해서 해당 특징을 넘김
---
### 6교시
- Input Gate: 기억 주입
    - 새롭게 기억에 넣을 가치가 있는것만 사용
    - 기억의 후보와 곱연산하여 기록할 가치가 있는 정보만 남김

- Update Cell state: Old & New
    - 현시점의 기억 = 과거에 대한 정돈 + 현재에 대한 정돈

- New Hidden state: from Memory & from Now
    - 과거 히든 스테이트와 현재 기억를 시그모이드함수를 통해 배출하는 값 생성
    - 통합된 기억을 -1과 1 사이로 squeeze(tanh)하여 배출값과 곱연산, 새로운 hidden state 생성
- 기억 C는 같은 LSTM 안에서 관리하는 것이고, 다음 레이어로 넘기는 값은 hidden state H다

#### 결론
- LSTM은 RNN에 비해 똑똑하고 뚱뚱한 금붕어다!
---