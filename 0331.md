# 0331
## 언어지능 딥러닝
### 1교시
### Review
- tokenizer에서 index 수치는 단순히 분류용이지 특별한 값이 아니다 (txt seq -> idx seq)
    - cat: 10, dog: 50일 때 cat 5개 모여서 dog가 되지 않는다
- embedding으로 index를 여러 개 숫자로 변환 => representation
    - 내 목적에 맞도록 error를 낮춤
- RNN -> LSTM -> GRU
- Bidirectional
- Conv1D, MaxPool1D: 시점 축소! 금붕어들에게 매우 효율적인 기능
- 각 레이어의 특징을 알면 본인의 의도를 모델링에 넣고 테스트해볼 수 있다!
---
### 2교시
- 전처리 규칙의 기준은 항상 트레이닝 셋 기준
---
### 3교시
### Seq to Seq
- 단어를 다른 단어로 - 번역
- Encoder와 Decoder
- Encoder - Context - Decoder
- Decoder의 initial state는?
- Encoder와 Decoder에서 문장 길이가 다른 이슈
    - start of speech, end of speech를 넣어 해결
    - sos: 지시용 token
    - eos: 모델 -> 사람 "나 끝났어" 알림
- Decoder 인풋에 정답이 아닌 값이 들어가면 학습이 이상해질 수 있으므로 예측값을 넣지 않고 정답만 넣음 -> Teacher Forcing
---
### 4교시
- 영어 -> 프랑스어
- 프랑스어 문장 데이터 앞 뒤에 sos, eos 넣음
- 학습할 땐 eos 제외, y값으로는 sos 제외하여 넣음
    - fra_seq[:, 1:], fra_seq[:, :-1]
- encoder LSTM 등에서 ``return_state=True``: 마지막 시점의 hidden state 추가 리턴
- decoder LSTM 등에서 ``initial_state=``: encoder LSTM에서 리턴한 hidden state를 초기값으로 설정. 그러므로 encoder / decoder 레이어 크기는 서로 같아야 shape error 발생하지 않음!
```python
# 혹시 이미 그려둔 그래프가 있다면 날려줘!
tf.keras.backend.clear_session()

# 영어 단어 집합의 크기 : 5965, (50000, 11)
# 프랑스어 단어 집합의 크기 : 10406, (50000, 19)
# 프랑스어 문장은 길이가 19이지만,
# 디코더의 인풋으로 넣을때는 맨 뒤의 <eos>를 떼고 길이 18의 문장을
# 디코더의 아웃풋은 맨 앞의 <eos>를 떼고 길이 18의 문장으로 준비해야 함.

# Encoder
enc_X = tf.keras.layers.Input(shape=[eng_pad.shape[1]])
enc_E = tf.keras.layers.Embedding(eng_vocab_size, 64)(enc_X) # 토큰수, 차원수
enc_S_full, enc_S = tf.keras.layers.GRU(256, return_sequences=True, return_state=True)(enc_E)
## 물론 지금은 enc_S_full은 사용하지 않는다.

# Decoder
dec_X = tf.keras.layers.Input(shape=[fra_pad.shape[1]-1])
dec_E = tf.keras.layers.Embedding(fra_vocab_size, 64)(dec_X) # 토큰수, 차원수
dec_H = tf.keras.layers.GRU(256, return_sequences=True)(dec_E, initial_state=enc_S)
dec_H = tf.keras.layers.Dense(256, activation="swish")(dec_H) # 없어도 상관은 없는 부분.
dec_Y = tf.keras.layers.Dense(fra_vocab_size, activation="softmax")(dec_H) # 매시점에서, 어떤 단어가 타당할지 분류 문제로 푸는 것

model = tf.keras.models.Model([enc_X, dec_X], dec_Y)
# 텍스트는 index이고(원핫인코딩을 안했고)
# 아웃풋레이어는 분류문제 처럼 노드가 준비되어 있다면
# sparse categorical crossentropy
model.compile(loss='sparse_categorical_crossentropy',
              optimizer = 'rmsprop',
              metrics=['accuracy'])
model.summary()
```
- 원핫 인코딩을 하지 않고 분류한다면 ``sparse_categorical_crossentropy`` 사용
    - 단어 집합이 크므로 메모리 절약을 위해!

- Attention
    - dot product: 유사할수록 양의 값 커짐
    - dot product를 통해 attention score를 얻어 학습
