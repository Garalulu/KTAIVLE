# 0217
## 웹크롤링2
### 1교시
#### review
- Server - Client 관계 기억하기
- 주로 80 / 443 port로 요청
- GET: query로 전송. url에 데이터 포함
- POST: body로 전송. url에 데이터 포함 x
- requests 모듈로 데이터 수집
- selenium : 웹브라우저를 python 코드로 컨트롤해서 데이터 수집
- 정적 페이지(URL 변화) - html(str) > BeautifulSoup > css selector > DataFrame
- 동적 페이지(URL 변화 없이 데이터 수정) - json(str) > response.json() > DataFrame

##### 웹크롤링 절차
1. 웹 서비스 분석(개발자도구), API문서 : URL 찾기
2. request(URL) > response(data) : data(json(str), html(str))
3. data(json(str), html(str)) > response.json(), BeautifulSoup(css-selector) > DataFrame

##### 에러 발생시
- header 수정해서 데이터 요청
    - ``user-agent``, ``referer``
---
### 2교시

#### 직방에서 원룸 데이터 가져오기

##### 동 이름으로 위도 경도 구하기
```python
address = '운암동'
url = f'https://apis.zigbang.com/v2/search?leaseYn=N&q={address}&serviceType=원룸'
response = requests.get(url)
```

##### 위도, 경도로 geohash 알아내기
```python
geohash = geohash2.encode(lat, lng, precision=5)
```

##### geohash로 매물 아이디 가져오기
```python
url = f'https://apis.zigbang.com/v2/items?deposit_gteq=0&domain=zigbang&geohash={geohash}&needHasNoFiltered=true&\
    rent_gteq=0&sales_type_in=전세|월세&service_type_eq=원룸'
response = requests.get(url)
```

##### 매물 아이디로 매물 정보 가져오기
```python
url = 'https://apis.zigbang.com/v2/items/list'
params = {
    'domain': 'zigbang',
    'item_ids': ids[:900],
    'withCoalition': 'true',
}
response = requests.post(url, data=params)
```

#### Python스러운 코딩이란?
##### PEP document
- pep20, pep8(컨벤션 체크)
    - 문법(에러 발생 O, 코드 실행 X)
    - 컨벤션(에러 발생 X, 코드 실행 O)
- flake8: 코드 효율성 체크
- ``import this``로 python 철학 보기
```
The Zen of Python, by Tim Peters

Beautiful is better than ugly.
Explicit is better than implicit.
Simple is better than complex.
Complex is better than complicated.
Flat is better than nested.
Sparse is better than dense.
Readability counts.
Special cases aren't special enough to break the rules.
Although practicality beats purity.
Errors should never pass silently.
Unless explicitly silenced.
In the face of ambiguity, refuse the temptation to guess.
There should be one-- and preferably only one --obvious way to do it.
Although that way may not be obvious at first unless you're Dutch.
Now is better than never.
Although never is often better than *right* now.
If the implementation is hard to explain, it's a bad idea.
If the implementation is easy to explain, it may be a good idea.
Namespaces are one honking great idea -- let's do more of those!
```
---
### 3교시
- 스페셜 커맨드 ``%, %%`` - ipython에서 제공
    - ``%pwd``: 디렉토리 위치
    - ``%%time``: 해당 쉘 실행시간
- 쉘 커맨드 ``!`` - os system에서 제공

#### HTML 요소 보기
- 개발자 도구에서 화살표 버튼 클릭하면 화면에서 원하는 요소를 클릭해 찾을 수 있음(편집도 가능)
- 반대로 크롤링하려는 요소를 체크해 불러올 수도 있지 않을까?
- Document: 한 페이지
- Element: 하나의 레이아웃
- Tag: 엘리먼트 종류 정의
- Attribute: 시작태그에서 태그의 특정 기능을 하는 값
    - id: 유일한 값
    - class: 동일한 여러 개의 값 사용 가능, element 그룹핑
    - attr: id와 class를 제외한 속성
- Text: 시작태그와 끝태그 사이에 있는 문자열

---
### 4교시

#### CSS Selector
1. Element Selector
    - 엘리먼트 이름을 통해 선택
    - 가장 위에 있는 엘리먼트 선택됨
2. ID Selector
    - ID 이용해 선택
    - ``#(ID 이름)``으로 선택
    - 여러개 선택시 쉼표로 구분
3. Class Selector
    - ``.(Class 이름)``으로 선택
4. not Selector
    - ``.ds:not(.ds2)``으로 제외 선택
5. nth-child Selector
    - ``.ds:nth-child=""``으로 선택
6. Depth Selector
    - ``.contacts h1``으로 선택
    - 하위 모든 엘리먼트 선택됨
7. 바로 아래 Depth Selector
    - ``.contacts > h1``하위 엘리먼트 바로 아래 엘리먼트 선택

#### HTML Parsing
```python
from bs4 import BeautifulSoup 
```
`` dom = BeautifulSoup(response.text, 'html.parser')``

- 개발자 도구에서 원하는 엘리먼트 선택 후 ``Copy - Copy Selector``로 css select 가능
    - id가 없는 거 선택해서 찾아가세요

`` elements = dom.select('#nx_right_related_keywords > div > div.related_srch > ul > li')``
- css select (all)

`` keywords = [element.select_one('.tit').text for element in elements] ``
- 리스트 컨벤션으로 해당하는 목록 하나씩 저장
---
### 5교시

``element.select_one('.itemname').get('href')``
- 태그 안에 있는 속성값 가져올때 사용

##### Lazy mode
- 이미지를 불러오려 하는데 아래쪽 데이터에는 이미지가 로드되지 않았다?
- 네트워크 트래픽 위해 스크롤 내릴때만 데이터를 가져옴
- gmarket에서는 ``data-original``에 원본 이미지 링크 저장

``strip()``: 앞 뒤 공백 제거

##### 정규식 Regex
- 문자열 데이터를 특정 패턴으로 처리할 때 사용하는 문법
``` python
import re
price = '할인가11,800원'
re.findall('[0-9,]+', price)[0]
```
- 0부터 9, 쉼표(,)가 포함된 값 한 개 이상 표현
- result: `11,800`

정규식 및 lambda function, df 메소드 공부 필요

---
### 6교시

##### 디렉토리 생성
```python
# 이미지 파일 저장할 디렉토리
import os
dir_name = 'data'
if not os.path.exists(dir_name):
    os.makedirs(dir_name)
```

##### 이미지 다운로드
```python
response = requests.get(img_link)
with open('data/test.png', 'wb') as file:
    file.write(response.content)
```
```python
for idx, data in df[:5].iterrows():
    # 000, 001, 002, 003, ... 199.png 이름 형식
    filename = '0' * (3 - len(str(idx))) + str(idx)
    filename = f'data/{filename}.png'
    with open(filename, 'wb') as file:
        file.write(response.content)
```
- wb: 바이너리 파일로 저장
- `.content`로 받은 컨텐츠 읽어들이기

##### 이미지 전처리
- ``from PIL import Image as pil``
- ``pil.open('data/test.png')``: 이미지 로드

##### DF -> Tuple
- ``.iterrows()``: 데이터프레임을 (인덱스, 데이터프레임) 튜플 형태로 변환

#### Selenium
- 자동화를 목적으로 만들어진 다양한 브라우저와 언어를 지원하는 라이브러리
- **화면에 보이는 데이터**만 가져오는 특성

---
### 7교시

webdriver setting : https://github.com/SergeyPirogov/webdriver_manager

```python
%pip install webdriver-manager
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager

# 브라우저 화면에 띄우기
driver = webdriver.Chrome(ChromeDriverManager().install())
```

- ``.get(http)``: 해당 페이지로 이동

#### element 가져오기
- ``.find_element(By.CSS_SELECTOR, selector)``: element 하나 가져오기(CSS Selector 이용)
    - ``.find_elements``로 여러개 가져올수도 있음
    - ``.send_keys(text)``: 텍스트 입력
    - ``.click()``: 클릭 이벤트
- ``.find_element(By.XPATH, selector)``
    - copy Xpath
    - 보통 `Scrapy`를 사용하기 때문에 이게 주로 사용됨 <- 차후 공부 필요
    - ``//*[@id="shoji"]/div[2]/div/div[2]/header/div/div[1]``
        - `//`: 최상위 엘리먼트
        - `*`: 모든 하위 엘리먼트
        - `[@id="shoji"]`: 속성값으로 엘리먼트 선택
        - `/`: 한 단계 하위 엘리먼트
        - `div[n]`: 태그[n번째] // 1부터 시작함!

- ``.execute_script(script)``: 자바스크립트 코드 실행
    - ``window.scrollTo(200,300);``: 스크롤 위치 변경 스크립트
- ``.quit()``: 브라우저 종료

브라우저를 띄울 수 없는 환경에서는?
#### Headless
```python
options = webdriver.ChromeOptions()
options.add_argument('headless')
driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)
```

### 추가 학습
- Scrapy
- Dataframe method
- python다운 문법 쓰기 // pep20, pep8, flake8