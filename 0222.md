# 0222
### 1교시
#### review & 추가학습
- 데이터가 무조건 많다고 좋은 건 아니다
- 일정 학습 이후부터는 큰 변화 없음
- 다중공선성을 체크할 때는 모든 특성의 표준 편차를 1로 정규화 해야함
- VIF가 5 이상이면 다중공선성 존재 가능성 있으나, 이런 특성을 지워도 유의미한 변화가 없을 수 있음(거의 안함 실무에서?)
- Linear Regression은 x값이 많을수록 복잡해져 과적합이 발생한다(Ridge나 Lasso를 쓸 수 있는 가능성)
---
### 2교시
- 선형 회귀 복습
---
### 3교시
## K-nearest neighbor
- 회귀와 분류에 사용되는 지도 학습 알고리즘
    - 둘 다 쓰니까 모델 평가할때 헷갈리지 말기!
- 연산 속도가 느림
- k값(이웃 개수)에 따라 데이터를 다르게 예측할 수 있음(기본값=5)
    - 홀수로 설정: 짝수인 경우 과반수 이상이 나오지 않을 수 있음
- scailing 작업 필요

### Scailing
- Normalization
- Standardization
```python
# 모듈 불러오기
from sklearn.preprocessing import MinMaxScaler

# 정규화
scalar = MinMaxScaler()
scalar.fit(x_train)
x_train = scalar.transform(x_train)
x_test = scalar.transform(x_test)

# 데이터프레임 만들기 // 사람이 보기 위해 임의로 만듦
x_col = list(x)
x_train = pd.DataFrame(x_train, columns=x_col)
x_test = pd.DataFrame(x_test, columns=x_col)
```
---
### 4교시
- 회귀
    - ``from sklearn.neighbors import KNeighborsRegressor``
- 분류
    - ``from sklearn.neighbors import KNeighborsClassifier``

> target 값의 불균형도 꼭 체크해보자! (0: 500 / 1: 100)
---
### 5교시

## Decision Tree
- 특정 변수에 대한 의사결정 규칙을 나무가지가 뻗는 형태로 
- 분류, 회귀 모두 사용 가능
- 직관적이며 이해 및 설명 쉬움 // 화이트박스 모델
- 스무고개처럼 의미 있는 질문을 먼저 하는 것이 중요
- 과적합으로 성능 떨어지기 쉬움 // 트리 크기 제한 튜닝 필요
    - max_depth, min_samples_leaf, min_samples_split

### 불순도
- Gini Impurity
    - 완벽하게 분류: 0
    - 완벽하게 섞임: 0.5(binary 분류 기준)
    - 0~0.5 사이의 값으로 지니 불순도가 낮은 속성으로 의사결정 트리 노드 결정
- Entropy
    - 속성의 불순도
    - 정보 이득이 크다 = 어떤 속성으로 분할할 때 불순도가 줄어든다
    - 정보 이득이 가장 큰 속성부터 분할
---
### 6교시
#### 트리 파라미터
- max_depth
- min_samples_split
- min_samples_leaf
- max_feature
- max_leaf_node
#### 트리 시각화
- graphviz 활용
```python
# 시각화 모듈 불러오기
from sklearn.tree import export_graphviz
from IPython.display import Image

# 이미지 파일 만들기
export_graphviz(model,                                 # 모델 이름
                out_file='tree.dot',                   # 파일 이름 
                feature_names=x.columns,               # Feature 이름
                class_names=['die', 'survived'],       # Target Class 이름
                rounded=True,                          # 둥근 테두리
                precision=2,                           # 불순도 소숫점 자리수
                max_depth=3,                           # 출력할 깊이
                filled=True)                           # 박스 내부 채우기

# 파일 변환
!dot tree.dot -Tpng -otree.png -Gdpi=300

# 이미지 파일 표시
Image(filename='tree.png')
```
- 트리 읽는 방법
    - value: 실제 분류된 개수들
    - gini: 지니 불순도
    - sample: 개수
    - class: 분류된 상태

#### 변수 중요도 시각화
```python
# 데이터프레임 만들기 
perf_dic = {'feature':list(x), 'importance': model.feature_importances_}
df = pd.DataFrame(perf_dic)
df.sort_values(by='importance', ascending=True, inplace=True)

# 시각화
plt.figure(figsize=(5, 5))
plt.barh(df['feature'], df['importance'])
plt.show()
```
---
### 7교시
최적의 파라미터가 무엇일까? 튜닝으로 마지막 단추를 꿰라