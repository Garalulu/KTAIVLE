# 0313
## 시각지능 딥러닝
### 1교시
#### Review
- 딥러닝에서 무얼 했나?
    - Tensorflow.keras 사용
    - sequential api: 레이어를 순차적으로 연결
    - functional api: 레이어를 사슬처럼 조절해가며 연결(locally connected 가능. concatenate, add, ...)
    - 데이터 펼치기: reshape / flatten
    - 원핫인코딩: to_categorical
    - 다중 분류
        - loss: categorical_crossentropy
        - metrics: accuracy
        - optimizer: adam
    - 이진 분류
        - loss: binary_crossentropy
        - metrics: accuracy
        - optimizer: adam
    - 회귀
        - loss: mse
        - optimizer: adam

    - validation_split
    - EarlyStopping
---
### 2교시
- Dropout
    - 학습 과정에서 일정 비율 노드를 삭제
    - 초기값에 덜 민감하게
    - overfitting 방지
- Batch Normalization
    - 미니 배치가 학습되는 과정에서 배치 간 데이터의 분포가 확 바뀌는 것을 염려하여 만듦
    - 미니 배치도 스케일링을 해보자
    - hidden layer -> normalization -> activation이 필요하다 라고 주장 // 논쟁거리. 실제에서는 hidden layer -> activation -> normalization이 성능 더 좋았다

#### Review
- 하나의 노드는 앞 레이어 노드들의 값으로부터 뭔지 모르는 간단한 특징 따냄(현실의 무언가와 연결되는지는 알 수 없음)
- 첫 번째 히든 레이어 노드는 인풋 레이어의 노드에서 고려해볼 수 있는 간단한 특징을 잡아낼 수 있는 장치의 모임
- 다음 히든 레이어 노드는 간단하지만 앞 레이어보다 더 깊은 특징 추출
- 노드 수가 어느 정도 특징을 따길 바라는지, 레이어의 수는 어느 정도 계층으로 특징을 따길 바라는지 // 엔지니어의 의도가 담김
- 마지막 레이어는 최종 특징. 인풋과 아웃풋 관계에서 노이즈는 가능한 줄어있고, 신호는 가능한 증폭되어 있는 값

#### 컴퓨터가 이미지를 이해하게 하는 것
- 프로그램을 어떻게 만들어야 하나?
    - Rule-based programming (If, else)
    - Machine Learning
---
### 3교시
#### CNN Basics
- ImageNet: Fei Fei Li
- LeNet / AlexNet: Convolutional Neural Network
---
### 4교시
- 이미지 인식 성능이 그렇게 올라가지 않았다.
- 이미지를 1차원으로 평평하게 하는게 옳은 걸까?
- -> Convolution Layer 등장. Activation Map
#### Convolution Layer
- Layer 크기만큼 부분과 합성곱 진행
- Layer를 여러개 만들어 서로 가중치를 다르게 함
- 레이어 크기가 클수록 정보가 더 손실
- https://qph.cf2.quoracdn.net/main-qimg-b662a8fc3be57f76c708c171fcf29960
- stride를 주면 그 간격만큼 건너뛰어 합성곱
- Output size: (Input size - Filter size) / Stride + 1