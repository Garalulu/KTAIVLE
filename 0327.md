# 0327
## 언어지능 딥러닝
### 1교시
강사 - 김정훈 // Deep Learning Engineer edu.rayleigh@gmail.com
- ~2019: 빅데이터가 뭐지? 엔지니어들이 하나둘 배워가는 과정
- 2020: Digital Formation; 모든 회사/직원들이 소규모로 인공지능 프로젝트 하기 시작 -> 라벨링과 데이터 수집의 중요성 인식. 추천, 분류, 챗봇
    - 이걸 서비스에 어떻게 올리지?
    - 인공지능 연구자인가, 엔지니어인가?
    - ML-Ops
- 2022: 이제는 더 이상 연구자 / 엔지니어 모호함이 사라졌다
    - 왜 딥러닝이 시장을 점령하고 있나 를 이해해야 한다
- 연결주의를 연결주의답게, 해석하는 능력과 구도를 바라보는 눈
- 앞으로 어떤 관점으로 공부할지 의미를 파악해 봅시다

#### Text Mining 
- 텍스트 데이터 분석을 통해 비즈니스 의사결정에 도움이 되는 문장을 도출하는 과정
- 통계학? 어학? 
---
### 2교시
#### NLP
- Machine Translation, Information Retrieval, ...
- Text Mining이란 용어를 대체하고 있음
- 비즈니스 의사결정에 대한 우선순위가 낮아짐
    - 비즈니스는 경영자에게, 기술은 엔지니어에게. 분업의 출발
- 단일 loss로 내놓은 결과물이 모든 고전 알고리즘보다 성능이 좋다!

#### 중요하게 배울 것
- 2013: Word Embeddings
- 2013: NLP Neural Nets
- 2014: Seq to Seq Learning
- 2015: Attention

### Recurrent Neural Network (RNN)
- **과거의 정돈된 정보와 현재의 raw한 정보를 동시에 고려하여 모델의 목적에 맞는 representation을 하여 새로운 특징 제작**
- one to one, one to many, many to one, many to many..
---
### 3교시
- 앞 시점일수록 점점 반영이 줄어듦
- 뒷 시점부터 시작하는 모델도 있음
- **위 bold처리된 특징을 일관된 규칙으로 반복할 수 있음**
    - 가중치 재사용 = 일관된 규칙을 다시 적용할 수 있다
        - 시점만 다르고 동일한 위치에 있는 가중치는 재사용된다 (업뎃도 물론 됨)
    - Hidden Layer 대신 Hidden State, activation=tanh
---
### 4교시
- 1 epoch: 모든 데이터를 한 번씩 전부 이용하여 업데이트를 마쳤을 때
    - 미니 배치마다 여러번 가중치가 미분되어 계산되고, 이때마다 역전파 사용
- Input Data = [전체 Data 개수(row), 시점 수, 각 시점마다 한 번에 고려할 feature 수]
- Hidden State = [전체 Data 개수, 시점 수, feature 수]
- 마지막 시점의 값만 Output Layer로 전달
---
### 5교시
### Code
- 시계열 데이터 해석: Trend / Cycle
- sequence 전처리: [#, timestampsize, feature수]
```python
# 여기에 의미있는 기간(timestep을 지정해 봅시다.)
timestep= 21

# x의 데이터 구조를 3차원으로 만들어줘야 합니다.
x = np.array([data[i : i + timestep] for i in range( len(data) - timestep ) ])
y = np.array([data[i + timestep, -1] for i in range( len(data) - timestep ) ]) # timestep 다음 데이터의 마지막 컬럼
```
- x와 y는 시점을 분리해두어야 '예측'이 된다.
    - ex> 오늘까지 데이터로 내일 예측
---
### 6교시
- ``keras.layers.SimpleRNN(16, activation='tanh')``: RNN 세팅
    - return_sequences: False - 마지막 시점의 은닉 상태 / True - 모든 시점의 은닉 상태 리턴
    - RNN 층을 여러개 쌓을 때 True로 해야 각각 시점의 상태가 다음 층으로 넘어감
    - go_backwards: 나중에
- ``model.compile(loss='mae', optimizer='adam')  # mse 대비 급격한 학습을 막는데 종종 사용됨``
---